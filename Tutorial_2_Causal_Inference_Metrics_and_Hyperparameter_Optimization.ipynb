{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial 2: Causal Inference Metrics and Hyperparameter Optimization.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ren54OjCsCBA"
      },
      "source": [
        "These tutorials are licensed by [Bernard Koch](http://www.github.com/kochbj) under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-nc-sa/4.0/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELtcaUmUDh8x"
      },
      "source": [
        "# Tutorial 2: Causal Inference Metrics and Hyperparameter Optimization\n",
        " \n",
        "In the last tutorial we introduced the intuition behind representation learning for causal inference and built a simple TARNet model. Given that we are doing statistical inference, **proper model optimization is critical to achieving unbiased estimates.** In this tutorial I'll show you how to assess convergence of your model using Tensorboard, and do hyperparameter tuning using [Keras Tuner](https://keras-team.github.io/keras-tuner/). If you don't know what hyperparameters are, read the spoiler below.\n",
        " \n",
        "<details><summary>What are hyperparameters and why should we tune them?</summary>\n",
        " \n",
        "Hyperparameters are any parameters in your model that are not optimized by your loss function. In other words, they're the knobs you have to tune. To tune hyperparameters we need an objective score to compare between different hyperparameterizations. Often times this is the validation loss, but we can/should try something more sophisticated for causal inference (see below). A simple strategy called \"grid search\" is to exhaustively explore all possible hyperparameterizations and pick the best scoring one. Often times this is computationally intractible. Hyperparameter tuning packages like Keras Tuner instead implement more sophisticated algorithms for exploring the hyperparameter space (e.g., Random Search, Hyperband, Bayesian Optimization).\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6qjDawdD3J1"
      },
      "source": [
        "## Notation\n",
        "**Causal identification**\n",
        "\n",
        "- Observed covariates/features: $X$\n",
        "\n",
        "- Potential outcomes: $Y(0)$ and $Y(1)$\n",
        "\n",
        "- Treatment: $T$\n",
        "\n",
        "- Unobservable Individual Treatment Effect: $\\tau_i = Y_i(1) - Y_i(0)$\n",
        "\n",
        "- Average Treatment Effect: $ATE =\\mathbb{E}[Y_i(1)-Y_i(0)]= \\mathbb{E}[{\\tau_i}]$\n",
        "\n",
        "- Conditional Average Treatment Effect: $CATE(x) =\\mathbb{E}[Y_i(1)-Y_i(0)|X=x]$\n",
        "\n",
        "\n",
        "**Deep learning estimation**\n",
        "\n",
        "- Predicted outcomes: $\\hat{Y}(0)$ and $\\hat{Y}(1)$\n",
        "\n",
        "- Outcome modeling functions: $\\hat{Y}(T)=h(X,T)$ \n",
        "\n",
        "- Representation functions: $\\Phi(X)$\n",
        "\n",
        "- Propensity score function:\n",
        "$\\pi(X,T)=P(T|X)$ </br>*where $\\pi(X,1)=P(T=1|X)$ and $\\pi(X,0)=1-\\pi(X,1)$* \n",
        "\n",
        "- Loss functions: $\\mathcal{L}(true,predicted)$, with the mean squared error abbreviated $MSE$ and binary cross-entropy as $BCE$\n",
        "\n",
        "- Estimated CATE<sup>*</sup>: $\\hat{CATE_i} = \\hat{\\tau}_i = \\hat{Y_i}(1)-\\hat{Y_i}(0) = h(X,1)-h(X,0)$\n",
        "\n",
        "- Estimated ATE: $\\hat{ATE}=\\frac{1}{n}\\sum_{i=1}^n\\hat{CATE_i}$\n",
        "\n",
        "- Nearest-neighbor PEHE:\n",
        "$$PEHE_{nn}=\\frac{1}{N}\\sum_{i=1}^N{(\\underbrace{(1−2t_i)(y_i(t_i)−y_i^{nn}(1-t_i)}_{CATE_{nn}}−\\underbrace{(h(\\Phi(x),1)−h(\\Phi(x),0)))}_{\\hat{CATE}}}^2$$ for nearest neighbor $j$ of each unit $i$ in representation space such that $t_j\\neq t_i$:\n",
        "  $$y_i^{nn}(1-t_i) = \\min_{j\\in (1-T)}||\\Phi(x_i|t_i)-\\Phi(x_j|1-t_i)||_2$$\n",
        "\n",
        "\\* We define $\\hat{\\tau}_i = \\hat{CATE_i}$ because the we lack the covariates to estimate the ITE.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHh7vdZd1Vnt"
      },
      "source": [
        "# Part 1: Using Metrics and Tensorboard to Evaluate Models\n",
        "\n",
        "## Training metrics for causal inference\n",
        "\n",
        "Although our ultimate goal is to estimate the $\\hat{CATE}$, the loss function in TARNet only minimizes the factual error to estimate $\\hat{Y}$. This is a reflection of the fundamental problem of causal inference: we only observe one potential outcome for each unit.\n",
        "\n",
        "Within this literature, it is common practice to evaluate model performance on simulations using the Precision Estimation of Heterogeneous  Effects (PEHE) from [Hill, 2011](https://www.tandfonline.com/doi/abs/10.1198/jcgs.2010.08162?casa_token=b8-rfzagECIAAAAA:QeP7C4lKN6nZ7MkDjJHFrEberXopD9M5qPBMeBqbk84mI_8qGxj01ctgt4jdZtORpu9aZvpVRe07PA). PEHE measures the error in estimates of the $CATE$:\n",
        "\n",
        "$$PEHE=\\frac{1}{N}\\sum_{i=1}^N(CATE_i-\\hat{CATE_i})^2$$\n",
        "\n",
        "In order to select hyperparameters in real data, [Johansson et al., 2020](https://arxiv.org/pdf/2001.07426.pdf) propose to use a matching variant of $PEHE$ with the nearest Euclidean neighbor of each unit $i$ from the other treatment assignment group $y_i^{nn}$ as a counterfactual. If we identify the nearest neighbor $j$ of each unit $i$ in representation space such that $t_j\\neq t_i$ as\n",
        "\n",
        "  $$y_i^{nn}(1-t_i) = \\min_{j\\in (1-T)}||\\Phi(x_i|t_i)-\\Phi(x_j|1-t_i)||_2$$\n",
        " then,\n",
        "$$PEHE_{nn}=\\frac{1}{N}\\sum_{i=1}^N{(\\underbrace{(1−2t_i)(y_i(t_i)−y_i^{nn}(1-t_i)}_{CATE_{nn}}−\\underbrace{(h(\\Phi(x),1)−h(\\Phi(x),0)))}_{\\hat{CATE}}}^2$$\n",
        "If we take the square root of the $PEHE_{nn}$ then we get an approximation of the unit-level error.\n",
        "\n",
        "I think the intuition behind $\\sqrt{PEHE_{nn}}$ is solid. If our representation function $\\Phi$ is truly learning to balance the treated and control distributions, $CATE_{nn}$ should coarsely measure it.\n",
        "\n",
        "### Additional metrics for simulations (known counterfactuals)\n",
        "\n",
        "Since we know both potential outcomes we might also like to calculate bias in $\\hat{ATE}$ and $\\hat{CATE}$, as well as the actual $PEHE$,\n",
        "\n",
        "- $ATE_{bias} = |ATE-\\hat{ATE}|$\n",
        "- $CATE_{bias} = \\frac{1}{N}\\sum_{i=1}^N |CATE_i-\\hat{CATE_i}|$\n",
        "-  $\\sqrt{PEHE}=\\sqrt{\\frac{1}{N}\\sum_{i=1}^N(CATE_i-\\hat{CATE_i})^2}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TUbo0kw1_s7"
      },
      "source": [
        "## Reloading the Data\n",
        "\n",
        "Reload the IHDP data. for more information on the dataset see [this cell](https://colab.research.google.com/drive/1Zx0AkriygB_ws6qXjA7VfqebG-YMwbWl?authuser=2#scrollTo=jVhJelhqCMD7) from the previous tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a1oDDq-L-w5",
        "cellView": "form"
      },
      "source": [
        "#@title First load the data! (Click Play)\n",
        "import numpy as np\n",
        "!pip install scikit-learn==0.24.2\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.train.npz\n",
        "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.test.npz \n",
        "\n",
        "def load_IHDP_data(training_data,testing_data,i=7):\n",
        "    with open(training_data,'rb') as trf, open(testing_data,'rb') as tef:\n",
        "        train_data=np.load(trf); test_data=np.load(tef)\n",
        "        y=np.concatenate(   (train_data['yf'][:,i],   test_data['yf'][:,i])).astype('float32') #most GPUs only compute 32-bit floats\n",
        "        t=np.concatenate(   (train_data['t'][:,i],    test_data['t'][:,i])).astype('float32')\n",
        "        x=np.concatenate(   (train_data['x'][:,:,i],  test_data['x'][:,:,i]),axis=0).astype('float32')\n",
        "        mu_0=np.concatenate((train_data['mu0'][:,i],  test_data['mu0'][:,i])).astype('float32')\n",
        "        mu_1=np.concatenate((train_data['mu1'][:,i],  test_data['mu1'][:,i])).astype('float32')\n",
        "\n",
        "        data={'x':x,'t':t,'y':y,'t':t,'mu_0':mu_0,'mu_1':mu_1}\n",
        "        data['t']=data['t'].reshape(-1,1) #we're just padding one dimensional vectors with an additional dimension \n",
        "        data['y']=data['y'].reshape(-1,1)\n",
        "        \n",
        "        #rescaling y between 0 and 1 often makes training of DL regressors easier\n",
        "        data['y_scaler'] = StandardScaler().fit(data['y'])\n",
        "        data['ys'] = data['y_scaler'].transform(data['y'])\n",
        "\n",
        "    return data\n",
        "\n",
        "data=load_IHDP_data(training_data='./ihdp_npci_1-100.train.npz',testing_data='./ihdp_npci_1-100.test.npz')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsTQablc2-n0"
      },
      "source": [
        "## Adding our metrics to tensorflow\n",
        "\n",
        "If we use the `.fit` API's built-in metric system, we will be calculating our sample metrics within each mini-batch (e.g. for 64 units) which isn't very useful. Instead, we want to calculate our metrics on the full validation set or entire dataset all at once, at the end of ever epoch. We'll need to create a [custom callback](https://www.tensorflow.org/guide/keras/custom_callback) for that. A callback is code that runs dynamically in response to an event (e.g., a TF2 model finishing training). The procedure for subclassing Callbacks is very similar to how one would subclass custom layers or losses (both of which we do in subsequent tutorials).\n",
        "\n",
        "There are some tricky manipulations in `pdist2sq` and `find_ynn` but don't worry about those. The rest should be clear.\n",
        "\n",
        "Note that we've added the `tf.summary.scalar` lines to log these metrics for viewing in Tensorboard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDXVq2QXfRna"
      },
      "source": [
        "!pip install -q tensorflow==2.8.0\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import datetime\n",
        "#Colab command to allow us to run Colab in TF2\n",
        "%load_ext tensorboard "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1pELmlaISz0"
      },
      "source": [
        "from tensorflow.keras.callbacks import Callback\n",
        "\n",
        "def pdist2sq(x,y):\n",
        "    x2 = tf.reduce_sum(x ** 2, axis=-1, keepdims=True)\n",
        "    y2 = tf.reduce_sum(y ** 2, axis=-1, keepdims=True)\n",
        "    dist = x2 + tf.transpose(y2, (1, 0)) - 2. * x @ tf.transpose(y, (1, 0))\n",
        "    return dist\n",
        "\n",
        "'''\n",
        "def pdist2sq(A, B):\n",
        "    #helper for PEHEnn\n",
        "    #calculates squared euclidean distance between rows of two matrices  \n",
        "    #https://gist.github.com/mbsariyildiz/34cdc26afb630e8cae079048eef91865\n",
        "    # squared norms of each row in A and B\n",
        "    na = tf.reduce_sum(tf.square(A), 1)\n",
        "    nb = tf.reduce_sum(tf.square(B), 1)    \n",
        "    # na as a row and nb as a column vectors\n",
        "    na = tf.reshape(na, [-1, 1])\n",
        "    nb = tf.reshape(nb, [1, -1])\n",
        "    # return pairwise euclidean difference matrix\n",
        "    D = tf.sqrt(tf.maximum(na - 2*tf.matmul(A, B, False, True) + nb, 0.0))\n",
        "    return D\n",
        "'''\n",
        "\n",
        "#https://towardsdatascience.com/implementing-macro-f1-score-in-keras-what-not-to-do-e9f1aa04029d\n",
        "class Full_Metrics(Callback):\n",
        "    def __init__(self,data, verbose=0):   \n",
        "        super(Full_Metrics, self).__init__()\n",
        "        self.data=data #feed the callback the full dataset\n",
        "        self.verbose=verbose\n",
        "\n",
        "        #needed for PEHEnn; Called in self.find_ynn\n",
        "        self.data['o_idx']=tf.range(self.data['t'].shape[0])\n",
        "        self.data['c_idx']=self.data['o_idx'][self.data['t'].squeeze()==0] #These are the indices of the control units\n",
        "        self.data['t_idx']=self.data['o_idx'][self.data['t'].squeeze()==1] #These are the indices of the treated units\n",
        "    \n",
        "    def split_pred(self,concat_pred):\n",
        "        #this helps us keep ptrack of things so we don't make mistakes\n",
        "        preds={}\n",
        "        preds['y0_pred'] = self.data['y_scaler'].inverse_transform(concat_pred[:, 0].reshape(-1, 1))\n",
        "        preds['y1_pred'] = self.data['y_scaler'].inverse_transform(concat_pred[:, 1].reshape(-1, 1))\n",
        "        preds['phi'] = concat_pred[:, 2:]\n",
        "        return preds\n",
        "\n",
        "    def find_ynn(self, Phi):\n",
        "        #helper for PEHEnn\n",
        "        PhiC, PhiT =tf.dynamic_partition(Phi,tf.cast(tf.squeeze(self.data['t']),tf.int32),2) #separate control and treated reps\n",
        "        dists=tf.sqrt(pdist2sq(PhiC,PhiT)) #calculate squared distance then sqrt to get euclidean\n",
        "        yT_nn_idx=tf.gather(self.data['c_idx'],tf.argmin(dists,axis=0),1) #get c_idxs of smallest distances for treated units\n",
        "        yC_nn_idx=tf.gather(self.data['t_idx'],tf.argmin(dists,axis=1),1) #get t_idxs of smallest distances for control units\n",
        "        yT_nn=tf.gather(self.data['y'],yT_nn_idx,1) #now use these to retrieve y values\n",
        "        yC_nn=tf.gather(self.data['y'],yC_nn_idx,1)\n",
        "        y_nn=tf.dynamic_stitch([self.data['t_idx'],self.data['c_idx']],[yT_nn,yC_nn]) #stitch em back up!\n",
        "        return y_nn\n",
        "\n",
        "    def PEHEnn(self,concat_pred):\n",
        "        p = self.split_pred(concat_pred)\n",
        "        y_nn = self.find_ynn(p['phi']) #now its 3 plus because \n",
        "        cate_nn_err=tf.reduce_mean( tf.square( (1-2*self.data['t']) * (y_nn-self.data['y']) - (p['y1_pred']-p['y0_pred']) ) )\n",
        "        return cate_nn_err\n",
        "\n",
        "    def ATE(self,concat_pred):\n",
        "        p = self.split_pred(concat_pred)\n",
        "        return p['y1_pred']-p['y0_pred']\n",
        "\n",
        "    def PEHE(self,concat_pred):\n",
        "        #simulation only\n",
        "        p = self.split_pred(concat_pred)\n",
        "        cate_err=tf.reduce_mean( tf.square( ( (self.data['mu_1']-self.data['mu_0']) - (p['y1_pred']-p['y0_pred']) ) ) )\n",
        "        return cate_err \n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        concat_pred=self.model.predict(self.data['x'])\n",
        "        #Calculate Empirical Metrics        \n",
        "        ate_pred=tf.reduce_mean(self.ATE(concat_pred)); tf.summary.scalar('ate', data=ate_pred, step=epoch)\n",
        "        pehe_nn=self.PEHEnn(concat_pred); tf.summary.scalar('cate_nn_err', data=tf.sqrt(pehe_nn), step=epoch)\n",
        "        \n",
        "        #Simulation Metrics\n",
        "        ate_true=tf.reduce_mean(self.data['mu_1']-self.data['mu_0'])\n",
        "        ate_err=tf.abs(ate_true-ate_pred); tf.summary.scalar('ate_err', data=ate_err, step=epoch)\n",
        "        pehe =self.PEHE(concat_pred); tf.summary.scalar('cate_err', data=tf.sqrt(pehe), step=epoch)\n",
        "        out_str=f' — ate_err: {ate_err:.4f}  — cate_err: {tf.sqrt(pehe):.4f} — cate_nn_err: {tf.sqrt(pehe_nn):.4f} '\n",
        "        \n",
        "        if self.verbose > 0: print(out_str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACeqUKdM6d2f"
      },
      "source": [
        "## Running the Model\n",
        "Now reload the model, loss, and fitting boiler plate from last tutorial.\n",
        "We've made three minor changes. First, we return `phi` in `concat_pred`. Second, we add `FullMetrics` as a callback. Third, we add some code and a callback to save metrics for later viewing in Tensorboard:\n",
        "```\n",
        "!rm -rf ./logs/ \n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
        "file_writer.set_as_default()\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0USpaQdXzpKm"
      },
      "source": [
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Concatenate\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import Model\n",
        " \n",
        "def make_tarnet(input_dim, reg_l2):\n",
        "    '''\n",
        "    The first argument is the column dimension of our data.\n",
        "    It needs to be specified because the functional API creates a static computational graph\n",
        "    The second argument is the strength of regularization we'll apply to the output layers\n",
        "    '''\n",
        "    x = Input(shape=(input_dim,), name='input')\n",
        " \n",
        "    # REPRESENTATION\n",
        "    #in TF2/Keras it is idiomatic to instantiate a layer and pass its inputs on the same line unless the layer will be reused\n",
        "    #Note that we apply no regularization to the representation layers \n",
        "    phi = Dense(units=200, activation='elu', kernel_initializer='RandomNormal',name='phi_1')(x)\n",
        "    phi = Dense(units=200, activation='elu', kernel_initializer='RandomNormal',name='phi_2')(phi)\n",
        "    phi = Dense(units=200, activation='elu', kernel_initializer='RandomNormal',name='phi_3')(phi)\n",
        " \n",
        "    # HYPOTHESIS\n",
        "    y0_hidden = Dense(units=100, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y0_hidden_1')(phi)\n",
        "    y1_hidden = Dense(units=100, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y1_hidden_1')(phi)\n",
        " \n",
        "    # second layer\n",
        "    y0_hidden = Dense(units=100, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y0_hidden_2')(y0_hidden)\n",
        "    y1_hidden = Dense(units=100, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y1_hidden_2')(y1_hidden)\n",
        " \n",
        "    # third\n",
        "    y0_predictions = Dense(units=1, activation=None, kernel_regularizer=regularizers.l2(reg_l2), name='y0_predictions')(y0_hidden)\n",
        "    y1_predictions = Dense(units=1, activation=None, kernel_regularizer=regularizers.l2(reg_l2), name='y1_predictions')(y1_hidden)\n",
        " \n",
        "    #a convenience \"layer\" that concatenates arrays as columns in a matrix\n",
        "    #this time we'll return Phi as well to calculate cate_nn_err\n",
        "    concat_pred = Concatenate(1)([y0_predictions, y1_predictions, phi])\n",
        "    #the declarations above have specified the computational graph of our network, now we instantiate it\n",
        "    model = Model(inputs=x, outputs=concat_pred)\n",
        " \n",
        "    return model\n",
        " \n",
        "# every loss function in TF2 takes 2 arguments, a vector of true values and a vector predictions\n",
        "def regression_loss(concat_true, concat_pred):\n",
        "    #computes a standard MSE loss for TARNet\n",
        "    y_true = concat_true[:, 0] #get individual vectors\n",
        "    t_true = concat_true[:, 1]\n",
        " \n",
        "    y0_pred = concat_pred[:, 0]\n",
        "    y1_pred = concat_pred[:, 1]\n",
        " \n",
        "    #Each head outputs a prediction for both potential outcomes\n",
        "    #We use t_true as a switch to only calculate the factual loss\n",
        "    loss0 = tf.reduce_sum((1. - t_true) * tf.square(y_true - y0_pred))\n",
        "    loss1 = tf.reduce_sum(t_true * tf.square(y_true - y1_pred))\n",
        "    #note Shi uses tf.reduce_sum for her losses even though mathematically we should be using the mean\n",
        "    #tf.reduce_mean and tf.reduce_sum should be equivalent, but maybe having larger error gradients makes training easier?\n",
        "    return loss0 + loss1\n",
        " \n",
        "### MAIN CODE ####\n",
        " \n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
        "from tensorflow.keras.optimizers import SGD\n",
        " \n",
        "#make model\n",
        "tarnet_model=make_tarnet(data['x'].shape[1],.01)\n",
        " \n",
        "val_split=0.2\n",
        "batch_size=64\n",
        "verbose=True\n",
        "i = 0\n",
        "tf.random.set_seed(i)\n",
        "np.random.seed(i)\n",
        "yt = np.concatenate([data['ys'], data['t']], 1) #we'll use both y and t to compute the loss\n",
        " \n",
        "# Clear any logs from previous runs\n",
        "!rm -rf ./logs/ \n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
        "file_writer.set_as_default()\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        " \n",
        "sgd_callbacks = [\n",
        "        TerminateOnNaN(),\n",
        "        EarlyStopping(monitor='val_loss', patience=40, min_delta=0), \n",
        "        #40 is Shi's recommendation patience for this dataset, but you should tune for your data \n",
        "        ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n",
        "                          min_delta=0, cooldown=0, min_lr=0),\n",
        "        #This learning rate scheduling is quite agressive which seems good for this dataset\n",
        "        Full_Metrics(data,verbose),\n",
        "        tensorboard_callback\n",
        "    ]\n",
        "#optimizer hyperparameters\n",
        "sgd_lr = 1e-5\n",
        "momentum = 0.9\n",
        "tarnet_model.compile(optimizer=SGD(learning_rate=sgd_lr, momentum=momentum, nesterov=True),\n",
        "                    loss=regression_loss,\n",
        "                    metrics=regression_loss)\n",
        " \n",
        "tarnet_model.fit(x=data['x'],y=yt,\n",
        "                callbacks=sgd_callbacks,\n",
        "                validation_split=val_split,\n",
        "                epochs=300,\n",
        "                batch_size=batch_size,\n",
        "                verbose=verbose)\n",
        "print(\"Done!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tp933mT9vCM"
      },
      "source": [
        "## Evaluating the Model Using Tensorboard\n",
        "\n",
        "A key difference between neural networks and other machine learning models is that we avoid overfitting by stopping training once the validation error stops improving. We discussed in the last tutorial how this code is built into Tensorflow2 through callbacks. In general, it's good to check that your model has both converged and isn't being overfit in Tensorboard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dO3a9CScFUN0"
      },
      "source": [
        "%tensorboard --logdir logs/fit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmINnveVS18n"
      },
      "source": [
        "We'll focus on the logs of our losses and metrics in the \"Scalars\" tab. \n",
        "\n",
        "### Losses\n",
        "Let's begin with `epoch_regression_loss` (the difference between `epoch_regression_loss` and `epoch_loss` is that the latter includes $L_2$ penalties). Everything looks pretty textbook with training once validation loss has plateaued. Note that the smoothness of the plateau is because we chose very aggressive settings for `ReduceLROnPlateau`. This may be something you want to tune."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyiWzgufo584"
      },
      "source": [
        "### Metrics\n",
        "Let's start with our simulation metrics. The $\\sqrt{PEHE}$ (`cate_err`) score looks a lot like our validation loss which is good. This tells us that the model is learning to predict counterfactuals even if it only receives a factual loss. Our $ATE_{bias}$ (`ate_bias`) continues to decrease over time.\n",
        "\n",
        "The other key observation is that nearest neighbor estimates $\\sqrt{PEHE_{nn}}$ (`cate_nn_err`) are substantially biased. Because we are only using `cate_nn_err` to tune hyperparameters this isn't necessarily an issue, but we'll really need to check that `cate_nn_err` correlates with `cate_err` across hyperparamaterizations.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQLj657wOKGL"
      },
      "source": [
        "# Part 2: Hyperparameter Tuning for Statistical Estimators\n",
        "\n",
        "Now that we have metrics for model evaluation that are appropriate to causal inference, we can talk about hyperparameter optimization.\n",
        "\n",
        "**It doesn't matter what your model's theoretical guarantees are, if you do not appropriately tune your hyperparameters to your data you could get significantly biased estimates.** Beyond conventional hyperparameters (e.g. number of neurons), you should consider the choice of optimizer and it's settings as hyperparameters for a statistical inference. Here is a list of potentially tunable hyperparameters for TARNet:\n",
        "\n",
        "Regularization hyperparameters:\n",
        " - $\\lambda$ ($L_2$ regularization strength) for outcome layers\n",
        " - Dropout for outcome modeling layers\n",
        " - Batch normalization\n",
        "\n",
        "Architectural hyperparameters:\n",
        "  - Number of representation layers\n",
        "  - Number of neurons in a representation layer\n",
        "  - Number of output layers\n",
        "  - Number of neurons in an output layer\n",
        "  - Neuronal activation function (e.g. ELU, RELu, Sigmoid)\n",
        "\n",
        "Optimization Hyperparameters:\n",
        " - Choice of Optimizer (e.g. SGD, ADAM)\n",
        " - Optimizer Parameters (e.g. Momentum for SGD)\n",
        " - Learning Rate Scheduling Parameters\n",
        " - Early Stopping Paremeters\n",
        " - Batch Size\n",
        "\n",
        "We are now going to do a hyperparameter search using KerasTuner! We'll select hyperparameter settings that minimize the $\\sqrt{PEHE_{nn}}$.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcMr05p4PfRe"
      },
      "source": [
        "## Building a HyperModel using Keras Tuner\n",
        "\n",
        "Keras Tuner provides an elegant framework for compiling TF2 models with hyperparameters. We simply specify `hp.Int`, `hp.Choice` or `hp.Bool` for hyperparameters we wish to tune. Below, we are allowing the number of representation and hypothesis layers, the number of neurons in each layer, as well as the regularization strength to be hyperparameters. These ranges are loosely informed by suggestions from [Johansson et al., 2020](https://arxiv.org/pdf/2001.07426.pdf) for IHDP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZtKv-IZ2Exo"
      },
      "source": [
        "# Install Keras Tuner\n",
        "!pip install keras-tuner==1.0.4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66XXAlr69uXW"
      },
      "source": [
        "import keras_tuner as kt\n",
        "from keras_tuner.tuners import RandomSearch\n",
        "def make_hypertarnet(hp):\n",
        "    \"\"\"\n",
        "    Neural net predictive model. The dragon has three heads.\n",
        "    :param input_dim:\n",
        "    :param reg:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    # hp.Choice takes hyperparam name, list of options, and default\n",
        "    reg_l2=hp.Choice('l2',[.1,.01,.001],default=.01)\n",
        "    input_dim=25\n",
        "    inputs = Input(shape=(input_dim,), name='input')\n",
        "\n",
        "    # representation\n",
        "    rep_units = hp.Choice('rep_units', [50,100,200],default=200)\n",
        "    phi = Dense(units=rep_units, activation='elu', kernel_initializer='RandomNormal',name='phi_1')(inputs)\n",
        "    for i in range(hp.Int('rep_layers', 1, 2, default=1)):\n",
        "      #pretty nifty way to dynamically add more layers!\n",
        "      phi = Dense(units=rep_units, activation='elu', kernel_initializer='RandomNormal',name='phi_'+str(i+2))(phi)\n",
        "\n",
        "    # HYPOTHESIS\n",
        "    hyp_units = hp.Choice('hyp_units', [20,50,100,200],default=100)\n",
        "    y0_hidden = Dense(units=hyp_units, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y0_hidden_1')(phi)\n",
        "    y1_hidden = Dense(units=hyp_units, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y1_hidden_1')(phi)\n",
        "    for i in range(hp.Int('hyp_layers', 1, 3, default=2)):\n",
        "      y0_hidden = Dense(units=hyp_units, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y0_hidden_'+str(i+2))(y0_hidden)\n",
        "      y1_hidden = Dense(units=hyp_units, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y1_hidden_'+str(i+2))(y1_hidden)\n",
        "    \n",
        "    # OUTPUT\n",
        "    y0_predictions = Dense(units=1, activation=None, kernel_regularizer=regularizers.l2(reg_l2), name='y0_predictions')(y0_hidden)\n",
        "    y1_predictions = Dense(units=1, activation=None, kernel_regularizer=regularizers.l2(reg_l2), name='y1_predictions')(y1_hidden)\n",
        "\n",
        "    concat_pred = Concatenate(1)([y0_predictions, y1_predictions,phi])\n",
        "    model = Model(inputs=inputs, outputs=concat_pred)\n",
        "    \n",
        "    sgd_lr = 1e-5\n",
        "    momentum = 0.9\n",
        "    \n",
        "    optimizer=SGD(lr=sgd_lr, momentum=momentum, nesterov=True)\n",
        "    \n",
        "    model.compile(optimizer=SGD(lr=sgd_lr, momentum=momentum, nesterov=True),\n",
        "                      loss=regression_loss,\n",
        "                 metrics=regression_loss)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-nKhxMbQmib"
      },
      "source": [
        "## Tailoring Keras Tuner for our needs\n",
        "\n",
        "Because we wish to tune models using $\\sqrt{PEHE_{nn}}$ instead of the network's own loss function, we can't use the standard Keras Tuner framework. Instead, we subclass Keras Tuner and reimplement the `run_trial` method. Incidentally, this allows us to add other non-model (i.e., optimizer-related) parameters like the batch size and early-stopping patience.\n",
        "\n",
        " This code should look very familiar by now. The only differences is that we now have a `trial_id` for each parameter configuration which we need to use to save the model and Tensorboard logs. We also add an additional callback for saving these hyperparameter configurations in TensorBoard. Lastly the line,\n",
        "\n",
        "`self.oracle.update_trial(trial.trial_id, {'cate_nn_err': cate_nn_err})`\n",
        "\n",
        "reports the $\\sqrt{PEHE_{nn}}$ back to Keras Tuner so that it can compare models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjdkfNP2D37c"
      },
      "source": [
        "from keras_tuner.engine import tuner_utils\n",
        "from tensorboard.plugins.hparams import api as hparams_api\n",
        "!rm -rf my_dir\n",
        "\n",
        "class TarNetTuner(kt.Tuner):\n",
        "\n",
        "  def run_trial(self, trial,dataset,*fit_args, **fit_kwargs):\n",
        "      # *args and **kwargs in Python are positional (list) and keyword (dict) arguments\n",
        "      verbose = fit_kwargs['verbose']\n",
        "\n",
        "      log_dir=self.project_dir+'/trial_'+trial.trial_id\n",
        "      hp = trial.hyperparameters\n",
        "      \n",
        "      batch_size = hp.Int('batch_size', 128, 256, step=64, default=128)\n",
        "      stopping_patience=hp.Int('batch_size', 5, 15, step=5, default=5)\n",
        "\n",
        "      #some of this hacky code will hopefully go away as Keras Tuner get's more polished\n",
        "      hparams = tuner_utils.convert_hyperparams_to_hparams(trial.hyperparameters)\n",
        "      file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
        "      file_writer.set_as_default()\n",
        "      \n",
        "      tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "      hparams_callback = hparams_api.KerasCallback(\n",
        "                        writer=log_dir,\n",
        "                        hparams=hparams,\n",
        "                        #I prepend trial_ here to get it to save nicely. Hopefully will be fixed in future version of KT.\n",
        "                        trial_id='trial_'+trial.trial_id) \n",
        "      metrics_callback=Full_Metrics(dataset,verbose=verbose)\n",
        "      callbacks = [\n",
        "              TerminateOnNaN(),\n",
        "              EarlyStopping(monitor='val_loss', patience=stopping_patience, min_delta=0.0001),\n",
        "              ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n",
        "                                min_delta=0, cooldown=0, min_lr=0),\n",
        "              metrics_callback,\n",
        "              tensorboard_callback,\n",
        "              hparams_callback\n",
        "          ]\n",
        "\n",
        "      \n",
        "      model = self.hypermodel.build(hp)\n",
        "      model.fit(x=fit_args[0],y=fit_args[1],\n",
        "                 callbacks=callbacks,\n",
        "                  validation_split=fit_kwargs['validation_split'],\n",
        "                  epochs=fit_kwargs['epochs'],\n",
        "                  batch_size=batch_size, verbose=verbose)\n",
        "      \n",
        "      #give the metric to the hyperparameter optimization algorithm\n",
        "      concat_pred=model.predict(data['x'])\n",
        "      pehe_nn=metrics_callback.PEHEnn(concat_pred)\n",
        "      self.oracle.update_trial(trial.trial_id, {'cate_nn_err': tf.sqrt(pehe_nn)})\n",
        "      self.save_model(trial.trial_id, model)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sE90yhLQYh-E"
      },
      "source": [
        "## Running Keras Tuner\n",
        "\n",
        "Keras Tuner has three hyperparameter optimization strategies: Random Search, HyperBand, and Bayesian Optimization. Explaining (or attempting to explain) Bayesian Optimization, is beyond the scope of this tutorial, but we'll go with that. \n",
        "\n",
        "We'll simply explore a maximum of 10 configurations to keep this quick. In practice you probably want to set this to as many trials as your resources can accommodate and let Keras Tuner run overnight. At the end we'll print out the ID of the best trial.\n",
        "\n",
        "**WARNING:** This takes about 15 minutes. Make sure you are on a GPU or TPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6o1l7lDQz_J"
      },
      "source": [
        "\n",
        "tuner = TarNetTuner(\n",
        "    #the oracle is the hyperoptimization algorithm\n",
        "    oracle=kt.oracles.BayesianOptimization(\n",
        "        objective=kt.Objective('cate_nn_err', 'min'),\n",
        "        max_trials=10, #were trying to keep this quick for you.\n",
        "        #You probably want to do as many trials as your resources allow if you see variance between runs\n",
        "        seed=0    \n",
        "),\n",
        "        directory='my_dir',\n",
        "        project_name='helloworld',\n",
        "    hypermodel=make_hypertarnet\n",
        "    )\n",
        "tuner.search(data, data['x'],yt, epochs=300,validation_split=.2,verbose=2)\n",
        "\n",
        "best_trial=tuner.oracle.get_best_trials(num_trials=1)[0]\n",
        "print(\"BEST TRIAL ID:\",best_trial.trial_id)\n",
        "best_model=tuner.load_model(best_trial)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-Ikqa7ZNF18"
      },
      "source": [
        "## Examing Hyperparameters in Tensorboard\n",
        "\n",
        "Once KerasTuner is done, we can boot TensorBoard back up. This time we'll focus on the \"HPARAMS\" tab. In the \"Table View\" you can compare the best trial to others on the metrics we looked at before. The \"Parallel Cordinates View\" and \"Scatter Plot Matrix View\" have more information though.\n",
        "\n",
        "Let's check out the \"Parallel Cordinates View\" Here you can see trends across metrics and hyperparameterizations. To the far right are the metrics we really care about: `ate_pred`,`cate_nn_err`, and our true counterfactual errors `cate_err`.\n",
        "\n",
        "First, we note that while the correlation between `cate_nn_err` and `cate_err` isn't perfect, there is basically a cohort of good `cate_nn_err` that correspond with good `cate_err`. This seems to correspond with having two representation layers. Given that we won't know `cate_err` with real data, this makes `cate_nn_err` seem like a reasonable choice for hyperparameter optimization, although we should probably run multiple runs of each model before choosing a final model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PR2VABpBdGAc"
      },
      "source": [
        "%tensorboard --logdir my_dir/helloworld/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8ZJOwQgdPHm"
      },
      "source": [
        "# That's it!\n",
        "\n",
        "Some final thoughts on hyperparameter tuning: I think optimizer settings (early stopping, learning rate scheduling) may be more important hyperparameters than any architectural changes. Second, while hyperparameter optimzation is important, I'll emphasize that the differences between our best and worst models are pretty small, especially compared to non-neural network estimators like linear regression or causal forests.\n",
        "\n",
        "This concludes the tutorial on evaluation and hyperparameter optimization. In this tutorial we:\n",
        "\n",
        "- Introduced causal inference-specific metrics for DL models\n",
        "\n",
        "- Wrote a custom callback in Tensorflow\n",
        "\n",
        "- Learned how to evaluate our models in TensorBoard\n",
        "\n",
        "- Learned how to tune hyperparameters in KerasTuner and compare them in TensorBoard\n",
        "\n",
        "# Up Next...\n",
        "\n",
        " In the next tutorial we'll implement some more sophisticated models that do not just rely on representation learning for balancing and have some consistency guarantees."
      ]
    }
  ]
}