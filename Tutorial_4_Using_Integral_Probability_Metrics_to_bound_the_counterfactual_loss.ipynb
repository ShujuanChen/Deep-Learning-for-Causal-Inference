{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tutorial 4: Using Integral Probability Metrics to bound the counterfactual loss",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6lEQGN9sI-S"
      },
      "source": [
        "These tutorials are licensed by [Bernard Koch](http://www.github.com/kochbj) under a [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License](http://creativecommons.org/licenses/by-nc-sa/4.0/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khRcBCxUpWR-"
      },
      "source": [
        "# Tutorial 4: Using Integral Probability Metrics to Bound the Counterfactual Loss\n",
        "\n",
        "Instead of using  semi-parametric corrections based on the propensity score to adjust ATE estimates, [Shalit et al., 2017](https://arxiv.org/abs/1606.03976), [Johansson et al. 2019](https://arxiv.org/abs/1903.03448), and [Johansson et al., 2020](https://arxiv.org/abs/2001.07426) take a different approach to encourage the representation function $\\Phi$ to explicitly learn treatment effects: integral probability metrics. \n",
        "\n",
        "Integral probability metrics (IPMs) are true metrics (symmetric and obey the triangle inequality unlike KL divergence) that measure the distance between two distributions. **The key idea here is that we can add an IPM as a loss in TARNET to more explicitly encourage the representation layers to balance the covariate distribution of the treated group $T$, and the covariate distribution of the control group $C$.** In recent years, IPMs have become very popular losses for generative adversarial networks.   \n",
        "\n",
        "Theoretically, this group has developed generalization bounds for the $CATE$ that show that even though the counterfactual loss is unknowable, it can be bounded by the factual loss of $h$ and an IPM between the treated and control distributions. Their claims focus on two IPMs in particular:\n",
        "\n",
        "## Wasserstein Distance\n",
        "\n",
        "The Wasserstein or “Earth Mover’s” distance fits an interpretable ”map” (i.e.  a matrix) showing how to efficiently convert from one probability mass distribution to another. The  Wasserstein  distance  is  most  easily  understood  as  an  optimal  transport  problem  (i.e., a scenario where we want to transport one distribution to another at minimum cost).  The nickname \"Earth mover’s distance” comes from the metaphor of shoveling dirt to terraform one landscape into another.  In the idealized case in which one distribution can be perfectly transformed into another, the Wasserstein map corresponds exactly to a perfect one-to-one matching on covariates strategy ([Kallus, 2016](https://arxiv.org/abs/1612.08321)). Because calculating the Wasserstein distance requires optimizing the map, we'll focus on the computationally simpler Maximum Mean Discrepancy metric. \n",
        "\n",
        "## Maximum Mean Discrepancy (MMD)\n",
        "\n",
        "The  maximum  mean  discrepancy (MMD)  is  the  normed  distance between the means of two distributions $T$ and $C$,  after a kernel function has transformed them into a high-dimensional reproducing kernel Hibbert Space (RKHS). It relies on the kernel trick to calculate distances in the RKHS. The metric is built on the intuition that there is no function that would have differing expected values for $T$ and $C$ in this high-dimensional space, if $T$ and $C$ are the same distribution. Formally we can define the MMD as\n",
        "\n",
        "$$MMD(T,C) = ||\\mathbb{E}_{X \\sim T}\\phi(X) - \\mathbb{E}_{X \\sim C}\\phi(X)||^2_{\\mathcal{H}}$$\n",
        "\n",
        "where $\\phi$ is associated with a reproducing kernel function $k$ such as the Gaussian radial basis function kernel: $$k(X,X')=\\exp({- \\frac{||X-X'||^2}{2\\sigma^2} }) $$\n",
        "\n",
        "From [Johansson et al., 2020](https://arxiv.org/abs/2001.07426), an unbiased estimator for the square of the MMD from a sample of size $m$ drawn from treated distribution $T$ and a sample of size $n$ drawn from control distribution $C$ is,\n",
        "\n",
        "$$\\hat{MMD}^2_k(T,C):=\\frac{1}{m(m-1)}\\sum_{i=1}^m\\sum_{i=j}^mk(x_i^T,x_j^T)-\\frac{2}{mn}\\sum_{i=1}^m\\sum_{i=j}^nk(x_i^T,x_j^C)+\\frac{1}{n(n-1)}\\sum_{i=1}^n\\sum_{i=j}^nk(x_i^C,x_j^C)$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFjT_fDuppay"
      },
      "source": [
        "## Notation\n",
        "**Causal identification**\n",
        "\n",
        "- Observed covariates/features: $X$\n",
        "\n",
        "- Potential outcomes: $Y(0)$ and $Y(1)$\n",
        "\n",
        "- Treatment: $T$\n",
        "\n",
        "- Unobservable Individual Treatment Effect: $\\tau_i = Y_i(1) - Y_i(0)$\n",
        "\n",
        "- Average Treatment Effect: $ATE =\\mathbb{E}[Y_i(1)-Y_i(0)]= \\mathbb{E}[{\\tau_i}]$\n",
        "\n",
        "- Conditional Average Treatment Effect: $CATE(x) =\\mathbb{E}[Y_i(1)-Y_i(0)|X=x]$\n",
        "\n",
        "\n",
        "**Deep learning estimation**\n",
        "\n",
        "- Predicted outcomes: $\\hat{Y}(0)$ and $\\hat{Y}(1)$\n",
        "\n",
        "- Outcome modeling functions: $\\hat{Y}(T)=h(X,T)$ \n",
        "\n",
        "- Representation functions: $\\Phi(X)$\n",
        "\n",
        "- Propensity score function:\n",
        "$\\pi(X,T)=P(T|X)$ </br>*where $\\pi(X,1)=P(T=1|X)$ and $\\pi(X,0)=1-\\pi(X,1)$* \n",
        "\n",
        "- Loss functions: $\\mathcal{L}(true,predicted)$, with the mean squared error abbreviated $MSE$ and binary cross-entropy as $BCE$\n",
        "\n",
        "- Estimated CATE<sup>*</sup>: $\\hat{CATE_i} = \\hat{\\tau}_i = \\hat{Y_i}(1)-\\hat{Y_i}(0) = h(X,1)-h(X,0)$\n",
        "\n",
        "- Estimated ATE: $\\hat{ATE}=\\frac{1}{n}\\sum_{i=1}^n\\hat{CATE_i}$\n",
        "\n",
        "- Nearest-neighbor PEHE:\n",
        "$$PEHE_{nn}=\\frac{1}{N}\\sum_{i=1}^N{(\\underbrace{(1−2t_i)(y_i(t_i)−y_i^{nn}(1-t_i)}_{CATE_{nn}}−\\underbrace{(h(\\Phi(x),1)−h(\\Phi(x),0)))}_{\\hat{CATE}}}^2$$ for nearest neighbor $j$ of each unit $i$ in representation space such that $t_j\\neq t_i$:\n",
        "  $$y_i^{nn}(1-t_i) = \\min_{j\\in (1-T)}||\\Phi(x_i|t_i)-\\Phi(x_j|1-t_i)||_2$$\n",
        "\n",
        "\\* We define $\\hat{\\tau}_i = \\hat{CATE_i}$ because the we lack the covariates to estimate the ITE.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7byTUsG71xr4"
      },
      "source": [
        "# Part 1: Coding up CFRNet using the MMD\n",
        "\n",
        "Johansson et al. call the variant of TARNet with an IPM loss  Counterfactual Regression Network (CFRNet). Architecturally CFRNet is identical to TARNet, but the loss function looks like this:\n",
        "\n",
        "\\begin{equation}\n",
        "\\min_{h,\\Phi,IPM}\\frac{1}{n}\\sum_{i=1}^n \\mathcal{L}(h(\\Phi(x_i),t_i),y_i) + \\lambda \\mathcal{R}(h)+\\alpha\\cdot IPM(\\Phi(X,|T=1),\\Phi(X|T=0))\\end{equation}\n",
        "where $\\mathcal{R}(h)$ is a model complexity term and $\\lambda$ and $\\alpha$ are hyperparameters. $IPM(\\Phi(X,|T=1),\\Phi(X|T=0))$ is an IPM distance between the covariate distributions of the treated and control distributions after they are projected into representation space.\n",
        "\n",
        "<figure><img src=https://github.com/kochbj/Deep-Learning-for-Causal-Inference/blob/main/images/CFRNet.png?raw=true width=\"900\"><figcaption>CFRNet architecture introduced in Shalit et al., 2017. Purple indicates inputs, orange indicates network layers, other colors indicate output layers, and white indicates outputs. The dashes between colored shapes indicate an unspecifed number of additional hidden layers. The dashed lines on the right indicate non-gradient, plug-in computations that occur after training.</a></figcaption></figure>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_o2BBef1THI"
      },
      "source": [
        "!pip install -q tensorflow==2.8.0\n",
        "import tensorflow as tf\n",
        "import numpy as np #numpy is the numerical computing package in python\n",
        "import datetime #we'll use dates to label our logs\n",
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nR8e_Kqyzq_l"
      },
      "source": [
        "First reload the data..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loq6MWJbIgft"
      },
      "source": [
        "!pip install scikit-learn==0.24.2\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.train.npz\n",
        "!wget -nc http://www.fredjo.com/files/ihdp_npci_1-100.test.npz \n",
        "\n",
        "def load_IHDP_data(training_data,testing_data,i=7):\n",
        "    with open(training_data,'rb') as trf, open(testing_data,'rb') as tef:\n",
        "        train_data=np.load(trf); test_data=np.load(tef)\n",
        "        y=np.concatenate(   (train_data['yf'][:,i],   test_data['yf'][:,i])).astype('float32') #most GPUs only compute 32-bit floats\n",
        "        t=np.concatenate(   (train_data['t'][:,i],    test_data['t'][:,i])).astype('float32')\n",
        "        x=np.concatenate(   (train_data['x'][:,:,i],  test_data['x'][:,:,i]),axis=0).astype('float32')\n",
        "        mu_0=np.concatenate((train_data['mu0'][:,i],  test_data['mu0'][:,i])).astype('float32')\n",
        "        mu_1=np.concatenate((train_data['mu1'][:,i],  test_data['mu1'][:,i])).astype('float32')\n",
        "\n",
        "        data={'x':x,'t':t,'y':y,'t':t,'mu_0':mu_0,'mu_1':mu_1}\n",
        "        data['t']=data['t'].reshape(-1,1) #we're just padding one dimensional vectors with an additional dimension \n",
        "        data['y']=data['y'].reshape(-1,1)\n",
        "        \n",
        "        #rescaling y between 0 and 1 often makes training of DL regressors easier\n",
        "        data['y_scaler'] = StandardScaler().fit(data['y'])\n",
        "        data['ys'] = data['y_scaler'].transform(data['y'])\n",
        "\n",
        "    return data\n",
        "\n",
        "data=load_IHDP_data(training_data='./ihdp_npci_1-100.train.npz',testing_data='./ihdp_npci_1-100.test.npz')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ql1zHbZpYIV"
      },
      "source": [
        "## Creating a custom MMD loss\n",
        "\n",
        "We'll begin by writing up MMD estimator described above in a custom loss object. The layer will take $\\Phi$ and $T$ as inputs, and output $\\hat{MMD^2}$. We'll implement the unbiased MMD estimator described above with the Guassian RBF function. Note that I've chosen to calculate losses with `tf.[reduce_mean](https://)n` here because there can be exploding gradients in weighted version of CFRNet (described below)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qxu3lgi3pXf_"
      },
      "source": [
        "def pdist2sq(x,y):\n",
        "    x2 = tf.reduce_sum(x ** 2, axis=-1, keepdims=True)\n",
        "    y2 = tf.reduce_sum(y ** 2, axis=-1, keepdims=True)\n",
        "    dist = x2 + tf.transpose(y2, (1, 0)) - 2. * x @ tf.transpose(y, (1, 0))\n",
        "    return dist\n",
        "\n",
        "from tensorflow.keras.losses import Loss\n",
        "\n",
        "class CFRNet_Loss(Loss):\n",
        "  #initialize instance attributes\n",
        "  def __init__(self, alpha=1.,sigma=1.):\n",
        "      super().__init__()\n",
        "      self.alpha = alpha # balances regression loss and MMD IPM\n",
        "      self.rbf_sigma=sigma #for gaussian kernel\n",
        "      self.name='cfrnet_loss'\n",
        "      \n",
        "  def split_pred(self,concat_pred):\n",
        "      #generic helper to make sure we dont make mistakes\n",
        "      preds={}\n",
        "      preds['y0_pred'] = concat_pred[:, 0]\n",
        "      preds['y1_pred'] = concat_pred[:, 1]\n",
        "      preds['phi'] = concat_pred[:, 2:]\n",
        "      return preds\n",
        "\n",
        "  def rbf_kernel(self, x, y):\n",
        "    return tf.exp(-pdist2sq(x,y)/tf.square(self.rbf_sigma))\n",
        "\n",
        "  def calc_mmdsq(self, Phi, t):\n",
        "    Phic, Phit =tf.dynamic_partition(Phi,tf.cast(tf.squeeze(t),tf.int32),2)\n",
        "\n",
        "    Kcc = self.rbf_kernel(Phic,Phic)\n",
        "    Kct = self.rbf_kernel(Phic,Phit)\n",
        "    Ktt = self.rbf_kernel(Phit,Phit)\n",
        "\n",
        "    m = tf.cast(tf.shape(Phic)[0],Phi.dtype)\n",
        "    n = tf.cast(tf.shape(Phit)[0],Phi.dtype)\n",
        "\n",
        "    mmd = 1.0/(m*(m-1.0))*(tf.reduce_sum(Kcc))\n",
        "    mmd = mmd + 1.0/(n*(n-1.0))*(tf.reduce_sum(Ktt))\n",
        "    mmd = mmd - 2.0/(m*n)*tf.reduce_sum(Kct)\n",
        "    return mmd * tf.ones_like(t)\n",
        "\n",
        "  def mmdsq_loss(self, concat_true,concat_pred):\n",
        "    t_true = concat_true[:, 1]\n",
        "    p=self.split_pred(concat_pred)\n",
        "    mmdsq_loss = tf.reduce_mean(self.calc_mmdsq(p['phi'],t_true))\n",
        "    return mmdsq_loss\n",
        "\n",
        "  def regression_loss(self,concat_true,concat_pred):\n",
        "      y_true = concat_true[:, 0]\n",
        "      t_true = concat_true[:, 1]\n",
        "      p = self.split_pred(concat_pred)\n",
        "      loss0 = tf.reduce_mean((1. - t_true) * tf.square(y_true - p['y0_pred']))\n",
        "      loss1 = tf.reduce_mean(t_true * tf.square(y_true - p['y1_pred']))\n",
        "      return loss0+loss1\n",
        "\n",
        "  def cfr_loss(self,concat_true,concat_pred):\n",
        "      lossR = self.regression_loss(concat_true,concat_pred)\n",
        "      lossIPM = self.mmdsq_loss(concat_true,concat_pred)\n",
        "      return lossR + self.alpha * lossIPM\n",
        "\n",
        "      #return lossR + self.alpha * lossIPM\n",
        "\n",
        "  #compute loss\n",
        "  def call(self, concat_true, concat_pred):        \n",
        "      return self.cfr_loss(concat_true,concat_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaHuslIbzzg1"
      },
      "source": [
        "## Metrics callback \n",
        "\n",
        "Paste our standard callback from previous tutorials..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aku3g_ohbd-D"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "#https://towardsdatascience.com/implementing-macro-f1-score-in-keras-what-not-to-do-e9f1aa04029d\n",
        "class Base_Metrics(Callback):\n",
        "    def __init__(self,data, verbose=0):   \n",
        "        super(Base_Metrics, self).__init__()\n",
        "        self.data=data #feed the callback the full dataset\n",
        "        self.verbose=verbose\n",
        "\n",
        "        #needed for PEHEnn; Called in self.find_ynn\n",
        "        self.data['o_idx']=tf.range(self.data['t'].shape[0])\n",
        "        self.data['c_idx']=self.data['o_idx'][self.data['t'].squeeze()==0] #These are the indices of the control units\n",
        "        self.data['t_idx']=self.data['o_idx'][self.data['t'].squeeze()==1] #These are the indices of the treated units\n",
        "    \n",
        "    def split_pred(self,concat_pred):\n",
        "        preds={}\n",
        "        preds['y0_pred'] = self.data['y_scaler'].inverse_transform(concat_pred[:, 0].reshape(-1, 1))\n",
        "        preds['y1_pred'] = self.data['y_scaler'].inverse_transform(concat_pred[:, 1].reshape(-1, 1))\n",
        "        preds['phi'] = concat_pred[:, 2:]\n",
        "        return preds\n",
        "\n",
        "    def find_ynn(self, Phi):\n",
        "        #helper for PEHEnn\n",
        "        PhiC, PhiT =tf.dynamic_partition(Phi,tf.cast(tf.squeeze(self.data['t']),tf.int32),2) #separate control and treated reps\n",
        "        dists=tf.sqrt(pdist2sq(PhiC,PhiT)) #calculate squared distance then sqrt to get euclidean\n",
        "        yT_nn_idx=tf.gather(self.data['c_idx'],tf.argmin(dists,axis=0),1) #get c_idxs of smallest distances for treated units\n",
        "        yC_nn_idx=tf.gather(self.data['t_idx'],tf.argmin(dists,axis=1),1) #get t_idxs of smallest distances for control units\n",
        "        yT_nn=tf.gather(self.data['y'],yT_nn_idx,1) #now use these to retrieve y values\n",
        "        yC_nn=tf.gather(self.data['y'],yC_nn_idx,1)\n",
        "        y_nn=tf.dynamic_stitch([self.data['t_idx'],self.data['c_idx']],[yT_nn,yC_nn]) #stitch em back up!\n",
        "        return y_nn\n",
        "\n",
        "    def PEHEnn(self,concat_pred):\n",
        "        p = self.split_pred(concat_pred)\n",
        "        y_nn = self.find_ynn(p['phi']) #now its 3 plus because \n",
        "        cate_nn_err=tf.reduce_mean( tf.square( (1-2*self.data['t']) * (y_nn-self.data['y']) - (p['y1_pred']-p['y0_pred']) ) )\n",
        "        return cate_nn_err\n",
        "\n",
        "    def ATE(self,concat_pred):\n",
        "        p = self.split_pred(concat_pred)\n",
        "        return p['y1_pred']-p['y0_pred']\n",
        "\n",
        "    def PEHE(self,concat_pred):\n",
        "        #simulation only\n",
        "        p = self.split_pred(concat_pred)\n",
        "        cate_err=tf.reduce_mean( tf.square( ( (self.data['mu_1']-self.data['mu_0']) - (p['y1_pred']-p['y0_pred']) ) ) )\n",
        "        return cate_err \n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        concat_pred=self.model.predict(self.data['x'])\n",
        "        #Calculate Empirical Metrics        \n",
        "        ate_pred=tf.reduce_mean(self.ATE(concat_pred)); tf.summary.scalar('ate', data=ate_pred, step=epoch)\n",
        "        pehe_nn=self.PEHEnn(concat_pred); tf.summary.scalar('cate_nn_err', data=tf.sqrt(pehe_nn), step=epoch)\n",
        "        \n",
        "        #Simulation Metrics\n",
        "        ate_true=tf.reduce_mean(self.data['mu_1']-self.data['mu_0'])\n",
        "        ate_err=tf.abs(ate_true-ate_pred); tf.summary.scalar('ate_err', data=ate_err, step=epoch)\n",
        "        pehe =self.PEHE(concat_pred); tf.summary.scalar('cate_err', data=tf.sqrt(pehe), step=epoch)\n",
        "        out_str=f' — ate_err: {ate_err:.4f}  — cate_err: {tf.sqrt(pehe):.4f} — cate_nn_err: {tf.sqrt(pehe_nn):.4f} '\n",
        "        \n",
        "        if self.verbose > 0: print(out_str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xr9OFlflywEu"
      },
      "source": [
        "## Building the model\n",
        "\n",
        "You'll notice that our hyperparameterization of TARNet is a little different this time. Even though we haven't been actually comparing models, I wanted to try and implement things closer to the authors' recommended settings for IHDP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ali1chuaJFNS"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Concatenate\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "from tensorflow.keras.metrics import binary_accuracy\n",
        "from tensorflow.keras.losses import Loss\n",
        "\n",
        "def make_tarnet(input_dim, reg_l2):\n",
        "\n",
        "    x = Input(shape=(input_dim,), name='input')\n",
        "\n",
        "    # representation\n",
        "    phi = Dense(units=25, activation='elu', kernel_initializer='RandomNormal',name='phi_1')(x)\n",
        "    phi = Dense(units=25, activation='elu', kernel_initializer='RandomNormal',name='phi_2')(phi)\n",
        "\n",
        "    # HYPOTHESIS\n",
        "    y0_hidden = Dense(units=25, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y0_hidden_1')(phi)\n",
        "    y1_hidden = Dense(units=25, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y1_hidden_1')(phi)\n",
        "\n",
        "    # second layer\n",
        "    y0_hidden = Dense(units=25, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y0_hidden_2')(y0_hidden)\n",
        "    y1_hidden = Dense(units=25, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y1_hidden_2')(y1_hidden)\n",
        "\n",
        "    # third\n",
        "    y0_predictions = Dense(units=1, activation=None, kernel_regularizer=regularizers.l2(reg_l2), name='y0_predictions')(y0_hidden)\n",
        "    y1_predictions = Dense(units=1, activation=None, kernel_regularizer=regularizers.l2(reg_l2), name='y1_predictions')(y1_hidden)\n",
        "\n",
        "    concat_pred = Concatenate(1)([y0_predictions, y1_predictions,phi])\n",
        "    model = Model(inputs=x, outputs=concat_pred)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr_8uuCnzE0N"
      },
      "source": [
        "Again. I'm doing things a little differently this time. We're going to use ADAM as the optimizer this time since it was used in the paper. We also have a bigger batch size this time because the IPM is very unstable if the batch size is too small (i.e., there are no treated units in the batch). Lastly, we're running for a few more epochs this time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vbgxiuq7pVSA"
      },
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau, TerminateOnNaN\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "#Colab command to allow us to run Colab in TF2\n",
        "%load_ext tensorboard \n",
        "\n",
        "val_split=0.2\n",
        "batch_size=100\n",
        "verbose=1\n",
        "i = 0\n",
        "tf.random.set_seed(i)\n",
        "np.random.seed(i)\n",
        "yt = np.concatenate([data['ys'], data['t']], 1)\n",
        "\n",
        "# Clear any logs from previous runs\n",
        "!rm -rf ./logs/ \n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
        "file_writer.set_as_default()\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0)\n",
        "\n",
        "#let's try ADAM this time\n",
        "adam_callbacks = [\n",
        "        TerminateOnNaN(),\n",
        "        EarlyStopping(monitor='val_loss', patience=2, min_delta=0.),\n",
        "        ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n",
        "                          min_delta=1e-8, cooldown=0, min_lr=0),\n",
        "        tensorboard_callback,\n",
        "        Base_Metrics(data,verbose=verbose)\n",
        "    ]\n",
        "\n",
        "\n",
        "cfrnet_model=make_tarnet(data['x'].shape[1],.01)\n",
        "cfrnet_loss=CFRNet_Loss(alpha=1.0)\n",
        "\n",
        "cfrnet_model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "                      loss=cfrnet_loss,\n",
        "                 metrics=[cfrnet_loss,cfrnet_loss.regression_loss,cfrnet_loss.mmdsq_loss])\n",
        "\n",
        "cfrnet_model.fit(x=data['x'],y=yt,\n",
        "                 callbacks=adam_callbacks,\n",
        "                  validation_split=val_split,\n",
        "                  epochs=500,\n",
        "                  batch_size=batch_size,\n",
        "                  verbose=verbose)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJW3BeNZHxqN"
      },
      "source": [
        "## Reviewing results in Tensorboard\n",
        "When we run TensorBoard we notice a couple of things. First that with $\\alpha=1$, the MMD IPM contributes only ~5-15% of the total gradient that the regression loss contributes, but grows over time to put an increasingly stronger penalty on the representations for not balancing the data. Whether we should use a bigger $\\alpha$ is an empirical question for hyperparameter tuning.\n",
        "\n",
        "The other thing you should notice is that while the IPM definitely shrinks initially, within a single batch it is VERY high variance due to the small batch size, and it's unclear whether it's making a difference. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSuF1hLVFVl_"
      },
      "source": [
        "%tensorboard --logdir logs/fit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YwuCuFX3PEP"
      },
      "source": [
        "# Part 2: Adding weights to CFRNet\n",
        "\n",
        "A limitation of the original CFRNet is that it does not provide consistency guarantees. In [Johansson et al. 2019](https://arxiv.org/abs/1903.03448), and [Johansson et al., 2020](https://arxiv.org/abs/2001.07426), the authors introduce estimated weights $\\pi(\\Phi(X),T)$ derived from the propensity score to provide consistency guarantees. Interestingly, they also use these weights to relax some of the overlap assumptions as long as the weights themselves obey the positivity assumption. These weights are used to adjust both the representations when calculating the IPM and predicted outcomes.\n",
        "<figure><img src=https://github.com/kochbj/Deep-Learning-for-Causal-Inference/blob/main/images/WeightedCFRNet.png?raw=true width=\"900\"><figcaption>Weighted CFRNet architecture introduced in Johannson et al., 2020. Purple indicates inputs, orange indicates network layers, other colors indicate output layers, and white indicates outputs. The dashes between colored shapes indicate an unspecifed number of additional hidden layers. The dashed lines on the right indicate non-gradient, plug-in computations that occur after training.</a></figcaption></figure>\n",
        "\n",
        "Weighted CFRNet minimizes the following loss function:\n",
        "\\begin{equation}\n",
        "\\min_{h,\\Phi,IPM,\\pi,\\lambda_h,\\lambda_w}\\frac{1}{n}\\sum_{i=1}^n \\frac{P(t_i)}{\\pi(\\Phi(x_i,t_i))}\\cdot\\mathcal{L}(h(\\Phi(x_i),t_i),y_i) + \\lambda_h \\mathcal{R}(h)+\\alpha\\cdot IPM(\\frac{P(1)}{\\pi(\\Phi(X,1))}\\cdot\\Phi(X,|T=1),\\frac{P(0)}{\\pi(\\Phi(X,0))}\\cdot\\Phi(X|T=0))+\\lambda_\\pi\\frac{||\\pi||_2}{n}\\end{equation}\n",
        "where $R(h)$ is a model complexity term and $\\lambda_h$, $\\lambda_\\pi$ and $\\alpha$ are hyperparameters.\n",
        "\n",
        "Applying the weights during the calculation of the IPM should smooth out some of the noisiness we saw from computing the IPM in tiny batches. This should reduce bias, even if weighting presents a potential variance tradeoff. The last term $\\lambda_\\pi\\frac{||\\pi||_2}{n}$ regularizes the weights to reduce variance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eizSABMAxwd"
      },
      "source": [
        "## Coding up weighted CFRNet\n",
        "\n",
        "This is virtually identical to the code above but we introduce `weights_l2` for $\\lambda_\\pi$, and adjust our loss function to weight the IPM calculations and outcome predictions. We also adjust the number of layers and neurons per Johansson et al.'s recommendations. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1093ml7FRDb"
      },
      "source": [
        "def make_weighted_cfrnet(input_dim, reg_l2, weights_l2):\n",
        "\n",
        "    x = Input(shape=(input_dim,), name='input')\n",
        "\n",
        "    # representation\n",
        "    phi = Dense(units=32, activation='elu', kernel_initializer='RandomNormal',name='phi_1')(x)\n",
        "    phi = Dense(units=16, activation='elu', kernel_initializer='RandomNormal',name='phi_2')(phi)\n",
        "\n",
        "    # HYPOTHESIS\n",
        "    y0_hidden = Dense(units=16, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y0_hidden_1')(phi)\n",
        "    y1_hidden = Dense(units=16, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y1_hidden_1')(phi)\n",
        "    t_hidden = Dense(units=32, activation='elu', kernel_regularizer=regularizers.l2(weights_l2),name='t_hidden_1')(phi)\n",
        "\n",
        "    # second layer\n",
        "    #y0_hidden = Dense(units=25, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y0_hidden_2')(y0_hidden)\n",
        "    #y1_hidden = Dense(units=25, activation='elu', kernel_regularizer=regularizers.l2(reg_l2),name='y1_hidden_2')(y1_hidden)\n",
        "    t_hidden = Dense(units=16, activation='elu', kernel_regularizer=regularizers.l2(weights_l2),name='t_hidden_2')(t_hidden)\n",
        "\n",
        "    # third\n",
        "    y0_predictions = Dense(units=1, activation=None, kernel_regularizer=regularizers.l2(reg_l2), name='y0_predictions')(y0_hidden)\n",
        "    y1_predictions = Dense(units=1, activation=None, kernel_regularizer=regularizers.l2(reg_l2), name='y1_predictions')(y1_hidden)\n",
        "    t_predictions = Dense(units=1, activation='sigmoid', kernel_regularizer=regularizers.l2(weights_l2), name='t_predictions')(t_hidden)\n",
        "\n",
        "    concat_pred = Concatenate(1)([y0_predictions, y1_predictions,t_predictions,phi])\n",
        "    model = Model(inputs=x, outputs=concat_pred)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "class Weighted_CFRNet_Loss(CFRNet_Loss):\n",
        "    #initialize instance attributes\n",
        "    def __init__(self, prob_treat,alpha=1.0,sigma=1.0):\n",
        "        super().__init__()\n",
        "        self.pT=prob_treat\n",
        "        self.alpha = alpha\n",
        "        self.rbf_sigma=sigma\n",
        "        self.name='weighted_cfrnet_loss'\n",
        "    \n",
        "    def split_pred(self,concat_pred):\n",
        "        #generic helper to make sure we dont make mistakes\n",
        "        preds={}\n",
        "        preds['y0_pred'] = concat_pred[:, 0]\n",
        "        preds['y1_pred'] = concat_pred[:, 1]\n",
        "        preds['t_pred'] = concat_pred[:, 2]\n",
        "        preds['t_pred'] = (preds['t_pred'] + 0.001) / 1.002\n",
        "        preds['phi'] = concat_pred[:, 3:]\n",
        "        return preds\n",
        "    \n",
        "    #for logging purposes only\n",
        "    def treatment_acc(self,concat_true,concat_pred):\n",
        "        t_true = concat_true[:, 1]\n",
        "        p = self.split_pred(concat_pred)\n",
        "        #Since this isn't used as a loss, I've used tf.reduce_mean for interpretability\n",
        "        return tf.reduce_mean(binary_accuracy(t_true, p['t_pred'], threshold=0.5))\n",
        "\n",
        "    def calc_weighted_mmdsq(self, Phi, t_true, t_pred):\n",
        "        t_predC, t_predT  =tf.dynamic_partition(t_pred,tf.cast(tf.squeeze(t_true),tf.int32),2) #propensity\n",
        "        PhiC, PhiT =tf.dynamic_partition(Phi,tf.cast(tf.squeeze(t_true),tf.int32),2) #representation\n",
        "        weightC=tf.expand_dims((1-self.pT)/(1-t_predC),axis=-1)\n",
        "        weightT=tf.expand_dims(self.pT/t_predT,axis=-1)\n",
        "\n",
        "        wPhiC = weightC * PhiC\n",
        "        wPhiT = weightT * PhiT\n",
        "\n",
        " \n",
        "        Kcc = self.rbf_kernel(wPhiC,wPhiC)\n",
        "        Kct = self.rbf_kernel(wPhiC,wPhiT)\n",
        "        Ktt = self.rbf_kernel(wPhiT,wPhiT)\n",
        "\n",
        "        m = tf.cast(tf.shape(PhiC)[0],Phi.dtype)\n",
        "        n = tf.cast(tf.shape(PhiT)[0],Phi.dtype)\n",
        "\n",
        "        mmd = 1.0/(m*(m-1.0))*(tf.reduce_sum(Kcc)-m)\n",
        "        mmd = mmd + 1.0/(n*(n-1.0))*(tf.reduce_sum(Ktt)-n)\n",
        "        mmd = mmd - 2.0/(m*n)*tf.reduce_sum(Kct)\n",
        "        return mmd * tf.ones_like(t_true)\n",
        "\n",
        "    def weighted_mmdsq_loss(self, concat_true,concat_pred):\n",
        "        t_true = concat_true[:, 1]\n",
        "        p=self.split_pred(concat_pred)\n",
        "        mmdsq = tf.reduce_mean(self.calc_weighted_mmdsq(p['phi'],t_true, p['t_pred']))\n",
        "        return mmdsq\n",
        "\n",
        "    def weights(self,concat_true,concat_pred):\n",
        "        p = self.split_pred(concat_pred)\n",
        "        weightT=tf.expand_dims(self.pT/p['t_pred'],axis=-1)\n",
        "        return tf.reduce_mean(weightT)\n",
        "\n",
        "    def weighted_regression_loss(self, concat_true, concat_pred):\n",
        "        y_true = concat_true[:, 0]\n",
        "        t_true = concat_true[:, 1]\n",
        "        p = self.split_pred(concat_pred)\n",
        "\n",
        "        weightC=tf.expand_dims((1-self.pT)/(1.-p['t_pred']),axis=-1)\n",
        "        weightT=tf.expand_dims(self.pT/p['t_pred'],axis=-1)\n",
        "\n",
        "        loss0 = tf.reduce_mean((1. - t_true) * tf.square(y_true - p['y0_pred'])*weightC)\n",
        "        loss1 = tf.reduce_mean(t_true * tf.square(y_true - p['y1_pred'])* weightT)\n",
        "        return loss0 + loss1\n",
        "\n",
        "    def call(self, concat_true, concat_pred):\n",
        "        return self.weighted_regression_loss(concat_true,concat_pred) + self.alpha * self.weighted_mmdsq_loss(concat_true,concat_pred)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fg3PzPFnBdcG"
      },
      "source": [
        "## Metrics callback\n",
        "We just need to adjust our metric callback to account for our propensity score predictions..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgfMuFxC-f6Q"
      },
      "source": [
        "class Weighted_Metrics(Base_Metrics):\n",
        "    def __init__(self,data, verbose=0):   \n",
        "        super().__init__(data,verbose)\n",
        "\n",
        "    def split_pred(self,concat_pred):\n",
        "        preds={}\n",
        "        preds['y0_pred'] = self.data['y_scaler'].inverse_transform(concat_pred[:, 0])\n",
        "        preds['y1_pred'] = self.data['y_scaler'].inverse_transform(concat_pred[:, 1])\n",
        "        preds['t_pred'] = concat_pred[:, 2]\n",
        "        preds['phi'] = concat_pred[:, 3:]\n",
        "\n",
        "        return preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFHfLx_gTb2N"
      },
      "source": [
        "val_split=0.2\n",
        "batch_size=100\n",
        "verbose=1\n",
        "i = 0\n",
        "tf.random.set_seed(i)\n",
        "np.random.seed(i)\n",
        "yt = np.concatenate([data['ys'], data['t']], 1)\n",
        "pT=data['t'][data['t']==1].shape[0]/data['t'].shape[0]\n",
        "print(\"Probability of treament:\", pT)\n",
        "# Clear any logs from previous runs\n",
        "!rm -rf ./logs/ \n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
        "file_writer.set_as_default()\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=0)\n",
        "adam_callbacks = [\n",
        "        TerminateOnNaN(),\n",
        "        EarlyStopping(monitor='val_loss', patience=2, min_delta=0.),\n",
        "        ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=verbose, mode='auto',\n",
        "                          min_delta=1e-8, cooldown=0, min_lr=0),\n",
        "        tensorboard_callback,\n",
        "        Weighted_Metrics(data,verbose=verbose)\n",
        "    ]\n",
        "\n",
        "\n",
        "sgd_lr = 1e-4\n",
        "momentum = 0.9\n",
        "\n",
        "weighted_cfrnet_model=make_weighted_cfrnet(data['x'].shape[1],.001,.1)\n",
        "cfrnet_loss=Weighted_CFRNet_Loss(prob_treat=pT,alpha=1.)\n",
        "\n",
        "weighted_cfrnet_model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "                      loss=cfrnet_loss,\n",
        "                 metrics=[cfrnet_loss,cfrnet_loss.weights,cfrnet_loss.treatment_acc,cfrnet_loss.weighted_mmdsq_loss])\n",
        "\n",
        "weighted_cfrnet_model.fit(x=data['x'],y=yt,\n",
        "                 callbacks=adam_callbacks,\n",
        "                  validation_split=val_split,\n",
        "                  epochs=1000,\n",
        "                  batch_size=batch_size,\n",
        "                  verbose=verbose)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tMlQDGxLUp8"
      },
      "source": [
        "## Reviewing results in Tensorboard\n",
        "\n",
        "The most noticeable thing in our logs is that the `mmdsq_loss` is significantly smoother than in the previous trial. FWIW, the authors also demonstrate in the paper that including the weights lessens the dependence on $\\alpha$ because the weights are learned adaptively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QpmYOuiWjX0u"
      },
      "source": [
        "%tensorboard --logdir logs/fit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rIsRHGQl-vZ"
      },
      "source": [
        "# Thats it!\n",
        "\n",
        "- In this tutorial we learned about integral probability metrics.\n",
        "\n",
        "- We applied an IPM loss to TARNet to create CFRNet.\n",
        "\n",
        "- We extended CFRNet with propensity weights.\n",
        "\n",
        "- We built a (weighted) CFRNet model and tested it on the IHDP data.\n",
        "\n",
        "## Thanks\n",
        "\n",
        "I hope you found these tutorials useful. If you have questions or would like me to do more (I've thought about adding GANITE and exploring how to create confidence intervals) please let me know through my [Github](github.com/kochbj) or the email on my [website](bernardjkoch.com). \n"
      ]
    }
  ]
}